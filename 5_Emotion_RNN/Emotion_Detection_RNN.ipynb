{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e65770eb",
   "metadata": {},
   "source": [
    "# TP 3 : D√©tection de sentiments avec un RNN - Mehdi ATTAOUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8b0cd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851e61e",
   "metadata": {},
   "source": [
    "## 1. Donn√©es d‚Äôapprentissage : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ef1f5199",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"Je suis tr√®s content\",\n",
    "    \"C'√©tait une belle journ√©e\",\n",
    "    \"Je suis d√©√ßu\",\n",
    "    \"C'√©tait horrible\",\n",
    "    \"J'adore ce film\",\n",
    "    \"Je d√©teste ce livre\"\n",
    "]\n",
    "\n",
    "sentiment = [1, 1, 0, 0, 1, 0]  \n",
    "# Step 2: Create the DataFrame\n",
    "data = pd.DataFrame({\n",
    "    'Phrase': phrases,\n",
    "    'Sentiment': sentiment\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5dc36b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je suis tr√®s content</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'√©tait une belle journ√©e</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je suis d√©√ßu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'√©tait horrible</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J'adore ce film</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Je d√©teste ce livre</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Phrase  Sentiment\n",
       "0       Je suis tr√®s content          1\n",
       "1  C'√©tait une belle journ√©e          1\n",
       "2               Je suis d√©√ßu          0\n",
       "3           C'√©tait horrible          0\n",
       "4            J'adore ce film          1\n",
       "5        Je d√©teste ce livre          0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85fc03",
   "metadata": {},
   "source": [
    "## 2. Pr√©traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213e7107",
   "metadata": {},
   "source": [
    "### importation de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b5a46653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mehdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3547df9",
   "metadata": {},
   "source": [
    "### tokenization avec expressions regulier pour gerer les ponctuations du langue francais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b38d583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je suis tr√®s content</td>\n",
       "      <td>1</td>\n",
       "      <td>[je, suis, tr√®s, content]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'√©tait une belle journ√©e</td>\n",
       "      <td>1</td>\n",
       "      <td>[c, √©tait, une, belle, journ√©e]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je suis d√©√ßu</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, suis, d√©√ßu]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'√©tait horrible</td>\n",
       "      <td>0</td>\n",
       "      <td>[c, √©tait, horrible]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J'adore ce film</td>\n",
       "      <td>1</td>\n",
       "      <td>[j, adore, ce, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Je d√©teste ce livre</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, d√©teste, ce, livre]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Phrase  Sentiment                           Tokens\n",
       "0       Je suis tr√®s content          1        [je, suis, tr√®s, content]\n",
       "1  C'√©tait une belle journ√©e          1  [c, √©tait, une, belle, journ√©e]\n",
       "2               Je suis d√©√ßu          0                 [je, suis, d√©√ßu]\n",
       "3           C'√©tait horrible          0             [c, √©tait, horrible]\n",
       "4            J'adore ce film          1             [j, adore, ce, film]\n",
       "5        Je d√©teste ce livre          0         [je, d√©teste, ce, livre]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "data['Tokens'] = data['Phrase'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f182ce92",
   "metadata": {},
   "source": [
    "### creation du vocabulaire, et convertir en sequence des indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ec566fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = data['Tokens'].explode()\n",
    "vocab_list = sorted(list(set(tokens)))\n",
    "word_to_index = {\"<PAD>\": 0}\n",
    "word_to_index.update({word: idx + 1 for idx, word in enumerate(vocab_list)})\n",
    "\n",
    "# Conversion en indices\n",
    "data['Token_Indices'] = data['Tokens'].apply(lambda tokens: [word_to_index[word] for word in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c56f66af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_Indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je suis tr√®s content</td>\n",
       "      <td>1</td>\n",
       "      <td>[je, suis, tr√®s, content]</td>\n",
       "      <td>[11, 14, 15, 5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'√©tait une belle journ√©e</td>\n",
       "      <td>1</td>\n",
       "      <td>[c, √©tait, une, belle, journ√©e]</td>\n",
       "      <td>[3, 17, 16, 2, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je suis d√©√ßu</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, suis, d√©√ßu]</td>\n",
       "      <td>[11, 14, 7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'√©tait horrible</td>\n",
       "      <td>0</td>\n",
       "      <td>[c, √©tait, horrible]</td>\n",
       "      <td>[3, 17, 9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J'adore ce film</td>\n",
       "      <td>1</td>\n",
       "      <td>[j, adore, ce, film]</td>\n",
       "      <td>[10, 1, 4, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Je d√©teste ce livre</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, d√©teste, ce, livre]</td>\n",
       "      <td>[11, 6, 4, 13]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Phrase  ...       Token_Indices\n",
       "0       Je suis tr√®s content  ...     [11, 14, 15, 5]\n",
       "1  C'√©tait une belle journ√©e  ...  [3, 17, 16, 2, 12]\n",
       "2               Je suis d√©√ßu  ...         [11, 14, 7]\n",
       "3           C'√©tait horrible  ...          [3, 17, 9]\n",
       "4            J'adore ce film  ...       [10, 1, 4, 8]\n",
       "5        Je d√©teste ce livre  ...      [11, 6, 4, 13]\n",
       "\n",
       "[6 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b92e7",
   "metadata": {},
   "source": [
    "### ajoutons une jeton pad = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "faa8b6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Token_Indices</th>\n",
       "      <th>Padded_Indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Je suis tr√®s content</td>\n",
       "      <td>1</td>\n",
       "      <td>[je, suis, tr√®s, content]</td>\n",
       "      <td>[11, 14, 15, 5]</td>\n",
       "      <td>[11, 14, 15, 5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'√©tait une belle journ√©e</td>\n",
       "      <td>1</td>\n",
       "      <td>[c, √©tait, une, belle, journ√©e]</td>\n",
       "      <td>[3, 17, 16, 2, 12]</td>\n",
       "      <td>[3, 17, 16, 2, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je suis d√©√ßu</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, suis, d√©√ßu]</td>\n",
       "      <td>[11, 14, 7]</td>\n",
       "      <td>[11, 14, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'√©tait horrible</td>\n",
       "      <td>0</td>\n",
       "      <td>[c, √©tait, horrible]</td>\n",
       "      <td>[3, 17, 9]</td>\n",
       "      <td>[3, 17, 9, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>J'adore ce film</td>\n",
       "      <td>1</td>\n",
       "      <td>[j, adore, ce, film]</td>\n",
       "      <td>[10, 1, 4, 8]</td>\n",
       "      <td>[10, 1, 4, 8, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Je d√©teste ce livre</td>\n",
       "      <td>0</td>\n",
       "      <td>[je, d√©teste, ce, livre]</td>\n",
       "      <td>[11, 6, 4, 13]</td>\n",
       "      <td>[11, 6, 4, 13, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Phrase  Sentiment  ...       Token_Indices      Padded_Indices\n",
       "0       Je suis tr√®s content          1  ...     [11, 14, 15, 5]  [11, 14, 15, 5, 0]\n",
       "1  C'√©tait une belle journ√©e          1  ...  [3, 17, 16, 2, 12]  [3, 17, 16, 2, 12]\n",
       "2               Je suis d√©√ßu          0  ...         [11, 14, 7]   [11, 14, 7, 0, 0]\n",
       "3           C'√©tait horrible          0  ...          [3, 17, 9]    [3, 17, 9, 0, 0]\n",
       "4            J'adore ce film          1  ...       [10, 1, 4, 8]    [10, 1, 4, 8, 0]\n",
       "5        Je d√©teste ce livre          0  ...      [11, 6, 4, 13]   [11, 6, 4, 13, 0]\n",
       "\n",
       "[6 rows x 5 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pad_sequences_numpy(sequences, max_len=None, pad_value=0):\n",
    "    if max_len is None:\n",
    "        max_len = max(len(seq) for seq in sequences)\n",
    "    padded = np.full((len(sequences), max_len), pad_value)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded[i, :len(seq)] = seq\n",
    "    return padded\n",
    "\n",
    "padded_np = pad_sequences_numpy(data['Token_Indices'].tolist(), pad_value=word_to_index[\"<PAD>\"])\n",
    "data['Padded_Indices'] = list(padded_np)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79291a62",
   "metadata": {},
   "source": [
    "## creation RNN simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, rnn_type=\"GRU\"):\n",
    "        super(SimpleRNNClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        \n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  \n",
    "        output, hidden = self.rnn(embedded) \n",
    "        \n",
    "        if isinstance(hidden, tuple): \n",
    "            hidden = hidden[0]\n",
    "\n",
    "        hidden = hidden.squeeze(0)  \n",
    "        out = self.fc(hidden)       \n",
    "        out = self.sigmoid(out)     \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19b328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convertir les donn√©es en tenseurs PyTorch\n",
    "X = torch.tensor(data[\"Padded_Indices\"], dtype=torch.long)\n",
    "y = torch.tensor(data['Sentiment'].values, dtype=torch.float32).unsqueeze(1)  \n",
    "\n",
    "# Cr√©er DataLoader\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ad4e8b",
   "metadata": {},
   "source": [
    "### Fonction Cout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba2c138f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "√âpoque 1/200 - Perte moyenne : 0.7149\n",
      "√âpoque 2/200 - Perte moyenne : 0.6776\n",
      "√âpoque 3/200 - Perte moyenne : 0.6531\n",
      "√âpoque 4/200 - Perte moyenne : 0.6247\n",
      "√âpoque 5/200 - Perte moyenne : 0.5661\n",
      "√âpoque 6/200 - Perte moyenne : 0.4978\n",
      "√âpoque 7/200 - Perte moyenne : 0.3835\n",
      "√âpoque 8/200 - Perte moyenne : 0.2458\n",
      "√âpoque 9/200 - Perte moyenne : 0.1203\n",
      "√âpoque 10/200 - Perte moyenne : 0.0614\n",
      "√âpoque 11/200 - Perte moyenne : 0.0257\n",
      "√âpoque 12/200 - Perte moyenne : 0.0126\n",
      "√âpoque 13/200 - Perte moyenne : 0.0063\n",
      "√âpoque 14/200 - Perte moyenne : 0.0036\n",
      "√âpoque 15/200 - Perte moyenne : 0.0023\n",
      "√âpoque 16/200 - Perte moyenne : 0.0017\n",
      "√âpoque 17/200 - Perte moyenne : 0.0014\n",
      "√âpoque 18/200 - Perte moyenne : 0.0011\n",
      "√âpoque 19/200 - Perte moyenne : 0.0010\n",
      "√âpoque 20/200 - Perte moyenne : 0.0009\n",
      "√âpoque 21/200 - Perte moyenne : 0.0008\n",
      "√âpoque 22/200 - Perte moyenne : 0.0007\n",
      "√âpoque 23/200 - Perte moyenne : 0.0007\n",
      "√âpoque 24/200 - Perte moyenne : 0.0006\n",
      "√âpoque 25/200 - Perte moyenne : 0.0006\n",
      "√âpoque 26/200 - Perte moyenne : 0.0006\n",
      "√âpoque 27/200 - Perte moyenne : 0.0005\n",
      "√âpoque 28/200 - Perte moyenne : 0.0005\n",
      "√âpoque 29/200 - Perte moyenne : 0.0005\n",
      "√âpoque 30/200 - Perte moyenne : 0.0005\n",
      "√âpoque 31/200 - Perte moyenne : 0.0005\n",
      "√âpoque 32/200 - Perte moyenne : 0.0005\n",
      "√âpoque 33/200 - Perte moyenne : 0.0004\n",
      "√âpoque 34/200 - Perte moyenne : 0.0004\n",
      "√âpoque 35/200 - Perte moyenne : 0.0004\n",
      "√âpoque 36/200 - Perte moyenne : 0.0004\n",
      "√âpoque 37/200 - Perte moyenne : 0.0004\n",
      "√âpoque 38/200 - Perte moyenne : 0.0004\n",
      "√âpoque 39/200 - Perte moyenne : 0.0004\n",
      "√âpoque 40/200 - Perte moyenne : 0.0004\n",
      "√âpoque 41/200 - Perte moyenne : 0.0004\n",
      "√âpoque 42/200 - Perte moyenne : 0.0003\n",
      "√âpoque 43/200 - Perte moyenne : 0.0003\n",
      "√âpoque 44/200 - Perte moyenne : 0.0003\n",
      "√âpoque 45/200 - Perte moyenne : 0.0003\n",
      "√âpoque 46/200 - Perte moyenne : 0.0003\n",
      "√âpoque 47/200 - Perte moyenne : 0.0003\n",
      "√âpoque 48/200 - Perte moyenne : 0.0003\n",
      "√âpoque 49/200 - Perte moyenne : 0.0003\n",
      "√âpoque 50/200 - Perte moyenne : 0.0003\n",
      "√âpoque 51/200 - Perte moyenne : 0.0003\n",
      "√âpoque 52/200 - Perte moyenne : 0.0003\n",
      "√âpoque 53/200 - Perte moyenne : 0.0003\n",
      "√âpoque 54/200 - Perte moyenne : 0.0003\n",
      "√âpoque 55/200 - Perte moyenne : 0.0003\n",
      "√âpoque 56/200 - Perte moyenne : 0.0003\n",
      "√âpoque 57/200 - Perte moyenne : 0.0003\n",
      "√âpoque 58/200 - Perte moyenne : 0.0003\n",
      "√âpoque 59/200 - Perte moyenne : 0.0002\n",
      "√âpoque 60/200 - Perte moyenne : 0.0002\n",
      "√âpoque 61/200 - Perte moyenne : 0.0002\n",
      "√âpoque 62/200 - Perte moyenne : 0.0002\n",
      "√âpoque 63/200 - Perte moyenne : 0.0002\n",
      "√âpoque 64/200 - Perte moyenne : 0.0002\n",
      "√âpoque 65/200 - Perte moyenne : 0.0002\n",
      "√âpoque 66/200 - Perte moyenne : 0.0002\n",
      "√âpoque 67/200 - Perte moyenne : 0.0002\n",
      "√âpoque 68/200 - Perte moyenne : 0.0002\n",
      "√âpoque 69/200 - Perte moyenne : 0.0002\n",
      "√âpoque 70/200 - Perte moyenne : 0.0002\n",
      "√âpoque 71/200 - Perte moyenne : 0.0002\n",
      "√âpoque 72/200 - Perte moyenne : 0.0002\n",
      "√âpoque 73/200 - Perte moyenne : 0.0002\n",
      "√âpoque 74/200 - Perte moyenne : 0.0002\n",
      "√âpoque 75/200 - Perte moyenne : 0.0002\n",
      "√âpoque 76/200 - Perte moyenne : 0.0002\n",
      "√âpoque 77/200 - Perte moyenne : 0.0002\n",
      "√âpoque 78/200 - Perte moyenne : 0.0002\n",
      "√âpoque 79/200 - Perte moyenne : 0.0002\n",
      "√âpoque 80/200 - Perte moyenne : 0.0002\n",
      "√âpoque 81/200 - Perte moyenne : 0.0002\n",
      "√âpoque 82/200 - Perte moyenne : 0.0002\n",
      "√âpoque 83/200 - Perte moyenne : 0.0002\n",
      "√âpoque 84/200 - Perte moyenne : 0.0002\n",
      "√âpoque 85/200 - Perte moyenne : 0.0002\n",
      "√âpoque 86/200 - Perte moyenne : 0.0002\n",
      "√âpoque 87/200 - Perte moyenne : 0.0002\n",
      "√âpoque 88/200 - Perte moyenne : 0.0002\n",
      "√âpoque 89/200 - Perte moyenne : 0.0002\n",
      "√âpoque 90/200 - Perte moyenne : 0.0001\n",
      "√âpoque 91/200 - Perte moyenne : 0.0001\n",
      "√âpoque 92/200 - Perte moyenne : 0.0001\n",
      "√âpoque 93/200 - Perte moyenne : 0.0001\n",
      "√âpoque 94/200 - Perte moyenne : 0.0001\n",
      "√âpoque 95/200 - Perte moyenne : 0.0001\n",
      "√âpoque 96/200 - Perte moyenne : 0.0001\n",
      "√âpoque 97/200 - Perte moyenne : 0.0001\n",
      "√âpoque 98/200 - Perte moyenne : 0.0001\n",
      "√âpoque 99/200 - Perte moyenne : 0.0001\n",
      "√âpoque 100/200 - Perte moyenne : 0.0001\n",
      "√âpoque 101/200 - Perte moyenne : 0.0001\n",
      "√âpoque 102/200 - Perte moyenne : 0.0001\n",
      "√âpoque 103/200 - Perte moyenne : 0.0001\n",
      "√âpoque 104/200 - Perte moyenne : 0.0001\n",
      "√âpoque 105/200 - Perte moyenne : 0.0001\n",
      "√âpoque 106/200 - Perte moyenne : 0.0001\n",
      "√âpoque 107/200 - Perte moyenne : 0.0001\n",
      "√âpoque 108/200 - Perte moyenne : 0.0001\n",
      "√âpoque 109/200 - Perte moyenne : 0.0001\n",
      "√âpoque 110/200 - Perte moyenne : 0.0001\n",
      "√âpoque 111/200 - Perte moyenne : 0.0001\n",
      "√âpoque 112/200 - Perte moyenne : 0.0001\n",
      "√âpoque 113/200 - Perte moyenne : 0.0001\n",
      "√âpoque 114/200 - Perte moyenne : 0.0001\n",
      "√âpoque 115/200 - Perte moyenne : 0.0001\n",
      "√âpoque 116/200 - Perte moyenne : 0.0001\n",
      "√âpoque 117/200 - Perte moyenne : 0.0001\n",
      "√âpoque 118/200 - Perte moyenne : 0.0001\n",
      "√âpoque 119/200 - Perte moyenne : 0.0001\n",
      "√âpoque 120/200 - Perte moyenne : 0.0001\n",
      "√âpoque 121/200 - Perte moyenne : 0.0001\n",
      "√âpoque 122/200 - Perte moyenne : 0.0001\n",
      "√âpoque 123/200 - Perte moyenne : 0.0001\n",
      "√âpoque 124/200 - Perte moyenne : 0.0001\n",
      "√âpoque 125/200 - Perte moyenne : 0.0001\n",
      "√âpoque 126/200 - Perte moyenne : 0.0001\n",
      "√âpoque 127/200 - Perte moyenne : 0.0001\n",
      "√âpoque 128/200 - Perte moyenne : 0.0001\n",
      "√âpoque 129/200 - Perte moyenne : 0.0001\n",
      "√âpoque 130/200 - Perte moyenne : 0.0001\n",
      "√âpoque 131/200 - Perte moyenne : 0.0001\n",
      "√âpoque 132/200 - Perte moyenne : 0.0001\n",
      "√âpoque 133/200 - Perte moyenne : 0.0001\n",
      "√âpoque 134/200 - Perte moyenne : 0.0001\n",
      "√âpoque 135/200 - Perte moyenne : 0.0001\n",
      "√âpoque 136/200 - Perte moyenne : 0.0001\n",
      "√âpoque 137/200 - Perte moyenne : 0.0001\n",
      "√âpoque 138/200 - Perte moyenne : 0.0001\n",
      "√âpoque 139/200 - Perte moyenne : 0.0001\n",
      "√âpoque 140/200 - Perte moyenne : 0.0001\n",
      "√âpoque 141/200 - Perte moyenne : 0.0001\n",
      "√âpoque 142/200 - Perte moyenne : 0.0001\n",
      "√âpoque 143/200 - Perte moyenne : 0.0001\n",
      "√âpoque 144/200 - Perte moyenne : 0.0001\n",
      "√âpoque 145/200 - Perte moyenne : 0.0001\n",
      "√âpoque 146/200 - Perte moyenne : 0.0001\n",
      "√âpoque 147/200 - Perte moyenne : 0.0001\n",
      "√âpoque 148/200 - Perte moyenne : 0.0001\n",
      "√âpoque 149/200 - Perte moyenne : 0.0001\n",
      "√âpoque 150/200 - Perte moyenne : 0.0001\n",
      "√âpoque 151/200 - Perte moyenne : 0.0001\n",
      "√âpoque 152/200 - Perte moyenne : 0.0001\n",
      "√âpoque 153/200 - Perte moyenne : 0.0001\n",
      "√âpoque 154/200 - Perte moyenne : 0.0001\n",
      "√âpoque 155/200 - Perte moyenne : 0.0001\n",
      "√âpoque 156/200 - Perte moyenne : 0.0001\n",
      "√âpoque 157/200 - Perte moyenne : 0.0001\n",
      "√âpoque 158/200 - Perte moyenne : 0.0001\n",
      "√âpoque 159/200 - Perte moyenne : 0.0001\n",
      "√âpoque 160/200 - Perte moyenne : 0.0001\n",
      "√âpoque 161/200 - Perte moyenne : 0.0001\n",
      "√âpoque 162/200 - Perte moyenne : 0.0001\n",
      "√âpoque 163/200 - Perte moyenne : 0.0001\n",
      "√âpoque 164/200 - Perte moyenne : 0.0001\n",
      "√âpoque 165/200 - Perte moyenne : 0.0001\n",
      "√âpoque 166/200 - Perte moyenne : 0.0001\n",
      "√âpoque 167/200 - Perte moyenne : 0.0001\n",
      "√âpoque 168/200 - Perte moyenne : 0.0001\n",
      "√âpoque 169/200 - Perte moyenne : 0.0001\n",
      "√âpoque 170/200 - Perte moyenne : 0.0001\n",
      "√âpoque 171/200 - Perte moyenne : 0.0001\n",
      "√âpoque 172/200 - Perte moyenne : 0.0001\n",
      "√âpoque 173/200 - Perte moyenne : 0.0001\n",
      "√âpoque 174/200 - Perte moyenne : 0.0001\n",
      "√âpoque 175/200 - Perte moyenne : 0.0001\n",
      "√âpoque 176/200 - Perte moyenne : 0.0001\n",
      "√âpoque 177/200 - Perte moyenne : 0.0001\n",
      "√âpoque 178/200 - Perte moyenne : 0.0001\n",
      "√âpoque 179/200 - Perte moyenne : 0.0001\n",
      "√âpoque 180/200 - Perte moyenne : 0.0001\n",
      "√âpoque 181/200 - Perte moyenne : 0.0001\n",
      "√âpoque 182/200 - Perte moyenne : 0.0001\n",
      "√âpoque 183/200 - Perte moyenne : 0.0001\n",
      "√âpoque 184/200 - Perte moyenne : 0.0001\n",
      "√âpoque 185/200 - Perte moyenne : 0.0001\n",
      "√âpoque 186/200 - Perte moyenne : 0.0001\n",
      "√âpoque 187/200 - Perte moyenne : 0.0001\n",
      "√âpoque 188/200 - Perte moyenne : 0.0001\n",
      "√âpoque 189/200 - Perte moyenne : 0.0001\n",
      "√âpoque 190/200 - Perte moyenne : 0.0001\n",
      "√âpoque 191/200 - Perte moyenne : 0.0001\n",
      "√âpoque 192/200 - Perte moyenne : 0.0001\n",
      "√âpoque 193/200 - Perte moyenne : 0.0001\n",
      "√âpoque 194/200 - Perte moyenne : 0.0001\n",
      "√âpoque 195/200 - Perte moyenne : 0.0001\n",
      "√âpoque 196/200 - Perte moyenne : 0.0001\n",
      "√âpoque 197/200 - Perte moyenne : 0.0001\n",
      "√âpoque 198/200 - Perte moyenne : 0.0001\n",
      "√âpoque 199/200 - Perte moyenne : 0.0001\n",
      "√âpoque 200/200 - Perte moyenne : 0.0001\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recr√©ons le mod√®le (au cas o√π)\n",
    "model = SimpleRNNClassifier(vocab_size=len(word_to_index), embedding_dim=16, hidden_dim=32, rnn_type=\"LSTM\")\n",
    "\n",
    "# Optimiseur\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Pour stocker les pertes\n",
    "losses = []\n",
    "\n",
    "# Entra√Ænement\n",
    "num_epochs = 200\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)               # pr√©dictions\n",
    "        loss = criterion(outputs, batch_y)     # calcul de la perte\n",
    "        loss.backward()                        # r√©tropropagation\n",
    "        optimizer.step()                       # mise √† jour des poids\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    losses.append(avg_loss)\n",
    "    print(f\"√âpoque {epoch+1}/{num_epochs} - Perte moyenne : {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cc6e7",
   "metadata": {},
   "source": [
    "### Fonction predection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f8c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(phrase, model, tokenizer, word_to_index, max_len=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "       \n",
    "        tokens = tokenizer.tokenize(phrase.lower())\n",
    "        \n",
    "    \n",
    "        indices = [word_to_index.get(word, 0) for word in tokens]\n",
    "        \n",
    "      \n",
    "        if max_len is None:\n",
    "            max_len = X.shape[1]\n",
    "        padded = indices + [0]*(max_len - len(indices))\n",
    "        padded = padded[:max_len]  \n",
    "        # 4. Convertir en tenseur\n",
    "        input_tensor = torch.tensor([padded], dtype=torch.long) \n",
    "        # 5. Pr√©diction\n",
    "        output = model(input_tensor)\n",
    "        prob = output.item()\n",
    "\n",
    "        # 6. Interpr√©tation\n",
    "        label = 1 if prob >= 0.5 else 0\n",
    "        return prob, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e60139f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'je suis heureux' ‚Üí Sentiment pr√©dit: 0 (proba=0.0097)\n",
      "Phrase: 'je suis triste' ‚Üí Sentiment pr√©dit: 0 (proba=0.0097)\n"
     ]
    }
   ],
   "source": [
    "phrase1 = \"je suis heureux\"\n",
    "phrase2 = \"je suis triste\"\n",
    "\n",
    "prob1, label1 = predict_sentiment(phrase1, model, tokenizer, word_to_index)\n",
    "prob2, label2 = predict_sentiment(phrase2, model, tokenizer, word_to_index)\n",
    "\n",
    "print(f\"Phrase: '{phrase1}' ‚Üí Sentiment pr√©dit: {label1} (proba={prob1:.4f})\")\n",
    "print(f\"Phrase: '{phrase2}' ‚Üí Sentiment pr√©dit: {label2} (proba={prob2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131ff68",
   "metadata": {},
   "source": [
    "### ajouter heureux et triste pour que notre model peut l'apprendre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a3560",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_phrases = [\n",
    "    \"Je suis heureux\",     \n",
    "    \"Je suis triste\"       \n",
    "]\n",
    "new_sentiments = [1, 0]\n",
    "\n",
    "\n",
    "new_data = pd.DataFrame({\n",
    "    'Phrase': new_phrases,\n",
    "    'Sentiment': new_sentiments\n",
    "})\n",
    "data_extended = pd.concat([data[['Phrase', 'Sentiment']], new_data], ignore_index=True)\n",
    "\n",
    "\n",
    "data_extended['Tokens'] = data_extended['Phrase'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "\n",
    "\n",
    "tokens = data_extended['Tokens'].explode()\n",
    "vocab_list = sorted(list(set(tokens)))\n",
    "word_to_index = {\"<PAD>\": 0}\n",
    "word_to_index.update({word: idx + 1 for idx, word in enumerate(vocab_list)})\n",
    "\n",
    "data_extended['Token_Indices'] = data_extended['Tokens'].apply(lambda tokens: [word_to_index[word] for word in tokens])\n",
    "\n",
    "padded_np = pad_sequences_numpy(data_extended['Token_Indices'].tolist(), pad_value=word_to_index[\"<PAD>\"])\n",
    "data_extended['Padded_Indices'] = list(padded_np)\n",
    "\n",
    "X_ext = torch.tensor(data_extended['Padded_Indices'].tolist(), dtype=torch.long)\n",
    "y_ext = torch.tensor(data_extended['Sentiment'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# üîπ DataLoaer\n",
    "dataset_ext = TensorDataset(X_ext, y_ext)\n",
    "loader_ext = DataLoader(dataset_ext, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaeebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "X = torch.tensor(data[\"Padded_Indices\"], dtype=torch.long)\n",
    "y = torch.tensor(data['Sentiment'].values, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017d66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Loss: 0.6984\n",
      "Epoch 21/200 - Loss: 0.0004\n",
      "Epoch 41/200 - Loss: 0.0002\n",
      "Epoch 61/200 - Loss: 0.0001\n",
      "Epoch 81/200 - Loss: 0.0001\n",
      "Epoch 101/200 - Loss: 0.0001\n",
      "Epoch 121/200 - Loss: 0.0001\n",
      "Epoch 141/200 - Loss: 0.0000\n",
      "Epoch 161/200 - Loss: 0.0000\n",
      "Epoch 181/200 - Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SimpleRNNClassifier(vocab_size=len(word_to_index), embedding_dim=16, hidden_dim=32, rnn_type=\"LSTM\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "\n",
    "losses = []\n",
    "num_epochs = 200\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader_ext:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader_ext)\n",
    "    losses.append(avg_loss)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd919d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 - Loss: 0.7067\n",
      "Epoch 21/200 - Loss: 0.0005\n",
      "Epoch 41/200 - Loss: 0.0003\n",
      "Epoch 61/200 - Loss: 0.0002\n",
      "Epoch 81/200 - Loss: 0.0001\n",
      "Epoch 101/200 - Loss: 0.0001\n",
      "Epoch 121/200 - Loss: 0.0001\n",
      "Epoch 141/200 - Loss: 0.0001\n",
      "Epoch 161/200 - Loss: 0.0000\n",
      "Epoch 181/200 - Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "model = SimpleRNNClassifier(vocab_size=len(word_to_index), embedding_dim=16, hidden_dim=32, rnn_type=\"LSTM\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "losses = []\n",
    "num_epochs = 200\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in loader_ext:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(loader_ext)\n",
    "    losses.append(avg_loss)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fc07d0",
   "metadata": {},
   "source": [
    "### on remarque que le model peut detecter heureux et triste "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b40b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'je suis heureux' ‚Üí Sentiment pr√©dit: 1 (proba=1.0000)\n",
      "Phrase: 'je suis triste' ‚Üí Sentiment pr√©dit: 0 (proba=0.0000)\n"
     ]
    }
   ],
   "source": [
    "phrase1 = \"je suis heureux\"\n",
    "phrase2 = \"je suis triste\"\n",
    "\n",
    "prob1, label1 = predict_sentiment(phrase1, model, tokenizer, word_to_index)\n",
    "prob2, label2 = predict_sentiment(phrase2, model, tokenizer, word_to_index)\n",
    "\n",
    "print(f\"Phrase: '{phrase1}' ‚Üí Sentiment pr√©dit: {label1} (proba={prob1:.4f})\")\n",
    "print(f\"Phrase: '{phrase2}' ‚Üí Sentiment pr√©dit: {label2} (proba={prob2:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af69c4bf",
   "metadata": {},
   "source": [
    "# pourquoi ajoutons jetons <pad>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783bac22",
   "metadata": {},
   "source": [
    "#### Parce que les phrases n‚Äôont pas toutes la m√™me longueur, mais le mod√®le a besoin que tout soit de la m√™me taille pour apprendre correctement. Alors on ajoute des <PAD> pour compl√©ter les phrases courtes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555f851",
   "metadata": {},
   "source": [
    "# 1 Pourquoi un RNN est utile ici ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f97ee3",
   "metadata": {},
   "source": [
    "#### Parce qu‚Äôun RNN gere bien les sequences, on sait bien que le text est tous lie entre eux , les mots ne sont pas indepandants, donc on veux un reseaux neuronnes qui garde l'antecedant pour serve le prochain , ce que RNN peux faire ,et un reseaux neuronnes normal NN va echouer car il oublie l'antecedan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f16ad5",
   "metadata": {},
   "source": [
    "## 2- Inconv√©nients d‚Äôun RNN simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04170e",
   "metadata": {},
   "source": [
    "#### 1- il ne pas pas appredre des relations complexe de text , car il utilise seulement une seule couche "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea72dc43",
   "metadata": {},
   "source": [
    "#### 2- il a un memoire courte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0618499",
   "metadata": {},
   "source": [
    "#### 3-peut pas generaliser , surtout qu'on a seulement 6 phrases , donc les nouvelles mots comme heureux et triste sont inconnue pour lui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db1ad1",
   "metadata": {},
   "source": [
    "## 3-Que se passerait-il si on avait des phrases beaucoup plus longues ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6321bbfb",
   "metadata": {},
   "source": [
    "#### - notre RNN va connaitre plus de mots "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30a65a",
   "metadata": {},
   "source": [
    "#### probleme de gradient : notre rnn simple et peux pas apprendre des relations complexes , donc on doit utiliser des extensions comme LSTM ou GRU pour resoudre ca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5e45c",
   "metadata": {},
   "source": [
    "#### - on va avoir un entrainement lent , car on va traite plus de sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d00635",
   "metadata": {},
   "source": [
    "#### il a perdre de precesion , cause il va trouver un difficulter de compredre le sens global "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfb57ec",
   "metadata": {},
   "source": [
    "#### solution : utlisation de plusieurs couche et des extensions du RNN , avec des robusts etape de pre-traitement de donnees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
